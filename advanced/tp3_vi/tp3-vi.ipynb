{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1296a3d",
   "metadata": {},
   "source": [
    "# TP3 - *Latent Dirichlet Allocation* et Inférence variationnelle \n",
    "\n",
    "## Estimation avancée - G3 SDIA\n",
    "\n",
    "Dans ce TP, on s'intéresse à la méthode \"inférence variationnelle\" (VI) qui permet d'approcher la loi a posteriori d'un modèle (généralement inconnue) par une autre loi plus simple (généralement un produit de lois bien connues). Nous allons l'appliquer à un modèle probabiliste pour des données textuelles, appelé *Latent Dirichlet Allocation* (LDA, qui n'a rien à voir avec la LDA *Linear Discriminant Analysis* du cours de ML).\n",
    "\n",
    "### Instructions\n",
    "\n",
    "1. Renommer votre notebook sous la forme `tp3_Nom1_Nom2.ipynb`, et inclure le nom du binôme dans le notebook. \n",
    "\n",
    "2. Votre code, ainsi que toute sortie du code, doivent être commentés !\n",
    "\n",
    "3. Déposer votre notebook sur Moodle dans la section prévue à cet effet avant la date limite : 23 Décembre 2023, 23h59."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "85d382bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import pickle as pkl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22423210",
   "metadata": {},
   "source": [
    "### Partie 0 - Introduction\n",
    "\n",
    "LDA is a popular probabilistic model for text data, introducted in [Blei et al. (2003)](https://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf). In this model, the posterior distribution is intractable, and we choose to resort to variational inference (note that a Gibbs sampler would be feasible as well, but would be very slow). In particular, the CAVI updates can be easily derived.\n",
    "\n",
    "In a few words, in LDA, each document is a mixture of topics, and each topic is a mixture of words. Uncovering those is the goal of *topic modeling*, and this is what we are going to do today. We will be using a collection of abstracts of papers published in JMLR (*Journal of Machine Learning Research*), one of the most prominent journals of the field.\n",
    "\n",
    "**Check the .pdf file describing the model.**\n",
    "The posterior is :\n",
    "$$p(\\boldsymbol{\\beta}, \\boldsymbol{\\theta}, \\mathbf{z} | \\mathcal{D}),$$\n",
    "which we are going to approximate in the following way :\n",
    "$$\\simeq \\left[ \\prod_{k=1}^K q(\\beta_k) \\right] \\left[ \\prod_{d=1}^D q(\\theta_d) \\right] \\left[ \\prod_{d=1}^D \\prod_{n=1}^{N_d} q(z_{dn}) \\right], $$\n",
    "with :\n",
    "* $q(\\beta_k)$ a Dirichlet distribution (of size V) with parameter $[\\lambda_{k1}, ...,\\lambda_{kV}]$\n",
    "* $q(\\gamma_d)$ a Dirichlet distribution (of size K) with parameter $[\\gamma_{d1}, ...,\\gamma_{dK}]$\n",
    "* $q(z_{dn})$ a Multinomial distribution (of size K) with parameter $[\\phi_{dn1}, ..., \\phi_{dnK}]$\n",
    "\n",
    "The updates are as follows :\n",
    "* $$\\lambda_{kv} = \\eta + \\sum_{d=1}^D \\sum_{n=1}^{N_d} w_{dnv} \\phi_{dnk} $$\n",
    "* $$\\gamma_{dk} = \\alpha + \\sum_{n=1}^{N_d} \\phi_{dnk}$$\n",
    "* $$ \\phi_{dnk} \\propto \\exp \\left( \\Psi(\\gamma_{dk}) + \\Psi(\\lambda_{k, w_{dn}}) - \\Psi(\\sum_{v=1}^V \\lambda_{kv}) \\right)$$\n",
    "\n",
    "$\\Psi$ is the digamma function, use `scipy.special.digamma`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c059097f",
   "metadata": {},
   "source": [
    "### Partie 1 - Les données"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40bf0e5b",
   "metadata": {},
   "source": [
    "The data is already prepared, see code below. We have a total of 1898 abstracts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "41348f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "jmlr_papers = pkl.load(open(\"jmlr.pkl\",\"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0694906",
   "metadata": {},
   "source": [
    "**Q1.** Fill in a list of keywords from the course, to see how many papers are about probabilistic ML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "321a5acc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 596 Bayesian papers out of 1898\n"
     ]
    }
   ],
   "source": [
    "bayesian_jmlr_papers = []\n",
    "\n",
    "for paper in jmlr_papers:\n",
    "    bayesian_keywords = [\"MCMC\",\"Sampling\",\"Bayesian\",\"inference\",\"Markov\",\"Monte Carlo\",\"Gibs\",\n",
    "                         \"prior\",\"posterior\",\"likelihood\",\"conjugate\"]\n",
    "    if any([kwd in paper[\"abstract\"] for kwd in bayesian_keywords]):\n",
    "        bayesian_jmlr_papers.append(paper)\n",
    "        \n",
    "print(\"There are\", str(len(bayesian_jmlr_papers))+\" Bayesian papers out of\", str(len(jmlr_papers)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a047cf9",
   "metadata": {},
   "source": [
    "Let us now preprocess the data. It is important to remove so-called \"stop-words\" like a, is, but, the, of, have... Scikit-learn will do the job for us. We will keep only the top-1000 words from the abstracts.\n",
    "\n",
    "As a result, we get the count matrix $\\mathbf{C}$ of size $D = 1898 \\times V = 1000$. $c_{dv}$ is the number of occurrences of word $v$ in document $d$. This compact representation is called \"bag-of-words\". Of course from $\\mathbf{C}$ you easily recover the words, since in LDA the order does not matter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "855d2359",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['100' '16' '17' '18' '949' '_blank' 'ability' 'able' 'abs' 'according'\n",
      " 'account' 'accuracy' 'accurate' 'achieve' 'achieved' 'achieves' 'action'\n",
      " 'actions' 'active' 'adaboost' 'adaptive' 'addition' 'additional'\n",
      " 'additive' 'address' 'advantage' 'advantages' 'agent' 'aggregation' 'al'\n",
      " 'algorithm' 'algorithmic' 'algorithms' 'allow' 'allowing' 'allows'\n",
      " 'alternative' 'analysis' 'analyze' 'applicable' 'application'\n",
      " 'applications' 'applied' 'apply' 'applying' 'approach' 'approaches'\n",
      " 'appropriate' 'approximate' 'approximately' 'approximation'\n",
      " 'approximations' 'arbitrary' 'art' 'article' 'artificial' 'associated'\n",
      " 'assume' 'assumed' 'assumption' 'assumptions' 'asymptotic'\n",
      " 'asymptotically' 'attributes' 'available' 'average' 'averaging' 'bandit'\n",
      " 'base' 'based' 'basic' 'basis' 'batch' 'bayes' 'bayesian' 'behavior'\n",
      " 'belief' 'benchmark' 'best' 'better' 'bias' 'bib' 'binary' 'block'\n",
      " 'boosting' 'bound' 'bounded' 'bounds' 'br' 'build' 'building' 'called'\n",
      " 'capture' 'carlo' 'case' 'cases' 'causal' 'central' 'certain' 'chain'\n",
      " 'challenge' 'challenging' 'characterization' 'characterize' 'choice'\n",
      " 'chosen' 'class' 'classes' 'classical' 'classification' 'classifier'\n",
      " 'classifiers' 'close' 'closed' 'cluster' 'clustering' 'clusters' 'code'\n",
      " 'coding' 'coefficient' 'coefficients' 'collection' 'color' 'com'\n",
      " 'combination' 'combine' 'combined' 'combining' 'common' 'commonly'\n",
      " 'community' 'comparable' 'compare' 'compared' 'comparing' 'comparison'\n",
      " 'comparisons' 'competitive' 'complete' 'complex' 'complexity' 'component'\n",
      " 'components' 'compression' 'computation' 'computational'\n",
      " 'computationally' 'compute' 'computed' 'computer' 'computing' 'concept'\n",
      " 'concepts' 'condition' 'conditional' 'conditions' 'confidence' 'consider'\n",
      " 'considered' 'consistency' 'consistent' 'consists' 'constant'\n",
      " 'constrained' 'constraint' 'constraints' 'construct' 'constructing'\n",
      " 'construction' 'context' 'continuous' 'contrast' 'contribution' 'control'\n",
      " 'converge' 'convergence' 'convex' 'core' 'correct' 'correlation'\n",
      " 'corresponding' 'cost' 'costs' 'covariance' 'criteria' 'criterion'\n",
      " 'cross' 'current' 'curve' 'dag' 'data' 'datasets' 'deal' 'decision'\n",
      " 'decomposition' 'deep' 'define' 'defined' 'definite' 'degree'\n",
      " 'demonstrate' 'demonstrated' 'density' 'dependence' 'dependencies'\n",
      " 'dependent' 'depends' 'derive' 'derived' 'descent' 'described'\n",
      " 'describes' 'design' 'designed' 'detection' 'determine' 'deterministic'\n",
      " 'develop' 'developed' 'dictionary' 'difference' 'differences' 'different'\n",
      " 'difficult' 'dimension' 'dimensional' 'dimensionality' 'dimensions'\n",
      " 'direct' 'directed' 'direction' 'directly' 'dirichlet' 'discovery'\n",
      " 'discrete' 'discriminant' 'discriminative' 'discuss' 'discussed'\n",
      " 'distance' 'distances' 'distributed' 'distribution' 'distributions'\n",
      " 'divergence' 'document' 'does' 'domain' 'domains' 'dual' 'dynamic'\n",
      " 'dynamics' 'easily' 'easy' 'edges' 'effect' 'effective' 'effectiveness'\n",
      " 'effects' 'efficiency' 'efficient' 'efficiently' 'elements' 'ell_1' 'em'\n",
      " 'embedding' 'empirical' 'empirically' 'enables' 'end' 'ensemble'\n",
      " 'entries' 'entropy' 'environment' 'epsilon' 'equivalence' 'equivalent'\n",
      " 'error' 'errors' 'especially' 'establish' 'established' 'estimate'\n",
      " 'estimated' 'estimates' 'estimating' 'estimation' 'estimator'\n",
      " 'estimators' 'et' 'euclidean' 'evaluate' 'evaluated' 'evaluation'\n",
      " 'evidence' 'exact' 'example' 'examples' 'exist' 'existence' 'existing'\n",
      " 'exists' 'expectation' 'expected' 'experimental' 'experiments' 'expert'\n",
      " 'experts' 'explicit' 'explicitly' 'exploit' 'exploiting' 'exploration'\n",
      " 'explore' 'exponential' 'expression' 'extend' 'extended' 'extension'\n",
      " 'extensions' 'extensive' 'fact' 'factor' 'factorization' 'factors'\n",
      " 'families' 'family' 'fast' 'faster' 'feature' 'features' 'field' 'fields'\n",
      " 'filtering' 'finally' 'finding' 'finite' 'fisher' 'fit' 'fitting' 'fixed'\n",
      " 'flexible' 'focus' 'following' 'font' 'form' 'formulation' 'formulations'\n",
      " 'framework' 'free' 'fully' 'function' 'functional' 'functions'\n",
      " 'fundamental' 'furthermore' 'future' 'games' 'gap' 'gaussian' 'gene'\n",
      " 'general' 'generalization' 'generalize' 'generalized' 'generally'\n",
      " 'generated' 'generating' 'generative' 'generic' 'geometric' 'geometry'\n",
      " 'github' 'given' 'gives' 'global' 'goal' 'good' 'gp' 'gradient' 'graph'\n",
      " 'graphical' 'graphs' 'gray' 'greedy' 'group' 'groups' 'guarantee'\n",
      " 'guarantees' 'hand' 'handle' 'hard' 'help' 'hidden' 'hierarchical' 'high'\n",
      " 'higher' 'highly' 'hilbert' 'hold' 'href' 'http' 'human' 'hypotheses'\n",
      " 'hypothesis' 'ica' 'idea' 'identify' 'identifying' 'ii' 'illustrate'\n",
      " 'image' 'images' 'implement' 'implementation' 'implementations'\n",
      " 'implemented' 'importance' 'important' 'improve' 'improved' 'improvement'\n",
      " 'improvements' 'improves' 'improving' 'include' 'includes' 'including'\n",
      " 'increasing' 'independence' 'independent' 'index' 'indicate' 'individual'\n",
      " 'induced' 'inference' 'infinite' 'influence' 'information' 'input'\n",
      " 'inputs' 'instance' 'instances' 'instead' 'interactions' 'interesting'\n",
      " 'interpretation' 'inthe' 'intractable' 'introduce' 'introduced'\n",
      " 'introduces' 'invariant' 'inverse' 'investigate' 'involves' 'involving'\n",
      " 'issue' 'issues' 'items' 'iteration' 'iterations' 'iterative' 'joint'\n",
      " 'kernel' 'kernels' 'key' 'knowledge' 'known' 'label' 'labeled' 'labels'\n",
      " 'lambda' 'language' 'large' 'larger' 'lasso' 'latent' 'layer' 'lead'\n",
      " 'leading' 'leads' 'learn' 'learned' 'learner' 'learners' 'learning'\n",
      " 'learns' 'level' 'li' 'library' 'like' 'likelihood' 'limit' 'limited'\n",
      " 'line' 'linear' 'linearly' 'literature' 'local' 'locally' 'log'\n",
      " 'logarithmic' 'logistic' 'long' 'loss' 'losses' 'low' 'lower' 'machine'\n",
      " 'machines' 'magnitude' 'main' 'make' 'makes' 'making' 'manifold' 'manner'\n",
      " 'map' 'margin' 'marginal' 'markov' 'match' 'matching' 'mathbb' 'mathcal'\n",
      " 'matlab' 'matrices' 'matrix' 'max' 'maximization' 'maximum' 'mcmc' 'mean'\n",
      " 'means' 'measure' 'measurements' 'measures' 'memory' 'message' 'method'\n",
      " 'methodology' 'methods' 'metric' 'minimal' 'minimax' 'minimization'\n",
      " 'minimize' 'minimizing' 'minimum' 'mining' 'missing' 'mixed' 'mixture'\n",
      " 'mixtures' 'model' 'modeling' 'models' 'monte' 'motivated' 'multi'\n",
      " 'multiclass' 'multiple' 'multivariate' 'mutual' 'naive' 'natural'\n",
      " 'naturally' 'nature' 'nbsp' 'near' 'nearest' 'necessary' 'need' 'needed'\n",
      " 'negative' 'neighbor' 'network' 'networks' 'neural' 'new' 'node' 'nodes'\n",
      " 'noise' 'noisy' 'non' 'nonlinear' 'nonparametric' 'norm' 'normal' 'norms'\n",
      " 'notion' 'novel' 'np' 'number' 'numbers' 'numerical' 'object' 'objective'\n",
      " 'objects' 'observation' 'observations' 'observed' 'obtain' 'obtained'\n",
      " 'obtaining' 'offers' 'ofthe' 'ones' 'online' 'onthe' 'open' 'operator'\n",
      " 'optimal' 'optimality' 'optimization' 'oracle' 'order' 'original'\n",
      " 'outperform' 'outperforms' 'output' 'outputs' 'overall' 'pac' 'package'\n",
      " 'pair' 'pairs' 'pairwise' 'paper' 'papers' 'parallel' 'parameter'\n",
      " 'parameters' 'parametric' 'partial' 'particular' 'particularly'\n",
      " 'partition' 'path' 'pattern' 'patterns' 'pca' 'pdf' 'penalized' 'penalty'\n",
      " 'perform' 'performance' 'performed' 'performs' 'perspective' 'phase'\n",
      " 'point' 'points' 'policies' 'policy' 'polynomial' 'popular' 'population'\n",
      " 'positive' 'possible' 'posterior' 'potential' 'power' 'powerful'\n",
      " 'practical' 'practice' 'precision' 'predict' 'predicting' 'prediction'\n",
      " 'predictions' 'predictive' 'predictor' 'predictors' 'presence' 'present'\n",
      " 'presented' 'presents' 'previous' 'previously' 'principal' 'principle'\n",
      " 'prior' 'priors' 'privacy' 'probabilistic' 'probabilities' 'probability'\n",
      " 'problem' 'problems' 'procedure' 'procedures' 'process' 'processes'\n",
      " 'processing' 'produce' 'product' 'program' 'programming' 'projection'\n",
      " 'propagation' 'properties' 'property' 'propose' 'proposed' 'provably'\n",
      " 'prove' 'provide' 'provided' 'provides' 'providing' 'purpose' 'python'\n",
      " 'quadratic' 'quality' 'queries' 'question' 'random' 'randomized' 'range'\n",
      " 'rank' 'ranking' 'rate' 'rates' 'real' 'recent' 'recently' 'recognition'\n",
      " 'recovery' 'reduce' 'reduced' 'reduces' 'reduction' 'regression' 'regret'\n",
      " 'regularization' 'regularized' 'reinforcement' 'related' 'relations'\n",
      " 'relationship' 'relationships' 'relative' 'relatively' 'relaxation'\n",
      " 'relevant' 'relies' 'rely' 'represent' 'representation' 'representations'\n",
      " 'represented' 'reproducing' 'require' 'required' 'requires' 'research'\n",
      " 'respect' 'response' 'restricted' 'result' 'resulting' 'results' 'reward'\n",
      " 'right' 'risk' 'rkhs' 'robust' 'robustness' 'role' 'rule' 'rules' 'run'\n",
      " 'running' 'sample' 'sampled' 'samples' 'sampling' 'scalable' 'scale'\n",
      " 'scales' 'scenarios' 'scheme' 'schemes' 'score' 'scoring' 'search'\n",
      " 'second' 'select' 'selection' 'semi' 'sense' 'sensitive' 'separation'\n",
      " 'sequence' 'sequences' 'sequential' 'series' 'set' 'sets' 'setting'\n",
      " 'settings' 'sgd' 'short' 'showing' 'shown' 'shows' 'showthat' 'signal'\n",
      " 'signals' 'significant' 'significantly' 'similar' 'similarity' 'simple'\n",
      " 'simulated' 'simulation' 'simulations' 'simultaneously' 'single'\n",
      " 'situations' 'size' 'sizes' 'small' 'smaller' 'smooth' 'social' 'soft'\n",
      " 'software' 'solution' 'solutions' 'solve' 'solved' 'solving' 'source'\n",
      " 'sources' 'space' 'spaces' 'sparse' 'sparsity' 'special' 'specific'\n",
      " 'specifically' 'specified' 'spectral' 'speed' 'squared' 'squares'\n",
      " 'stability' 'stable' 'stage' 'standard' 'state' 'stationary'\n",
      " 'statistical' 'statistics' 'step' 'steps' 'stochastic' 'strategies'\n",
      " 'strategy' 'strong' 'strongly' 'structural' 'structure' 'structured'\n",
      " 'structures' 'studied' 'studies' 'study' 'sub' 'subset' 'subsets'\n",
      " 'subspace' 'success' 'successfully' 'sufficient' 'suggest' 'suitable'\n",
      " 'sum' 'sup' 'supervised' 'support' 'surrogate' 'svm' 'svms' 'symmetric'\n",
      " 'synthetic' 'systems' 'table' 'takes' 'target' 'task' 'tasks' 'tdalign'\n",
      " 'technique' 'techniques' 'temporal' 'tensor' 'term' 'terms' 'test'\n",
      " 'testing' 'tests' 'text' 'thatthe' 'theorem' 'theoretic' 'theoretical'\n",
      " 'theoretically' 'theory' 'thispaper' 'threshold' 'tight' 'time' 'times'\n",
      " 'tool' 'toolbox' 'tools' 'topic' 'total' 'tr' 'tractable' 'trade'\n",
      " 'traditional' 'trained' 'training' 'transfer' 'treatment' 'tree' 'trees'\n",
      " 'true' 'type' 'types' 'typically' 'uncertainty' 'underlying'\n",
      " 'understanding' 'unified' 'uniform' 'universal' 'unknown' 'unlabeled'\n",
      " 'unlike' 'unsupervised' 'update' 'updates' 'upper' 'use' 'used' 'useful'\n",
      " 'usefulness' 'user' 'users' 'uses' 'using' 'usually' 'utility' 'v19'\n",
      " 'validation' 'value' 'valued' 'values' 'variable' 'variables' 'variance'\n",
      " 'variant' 'variants' 'variational' 'variety' 'various' 'varying' 'vector'\n",
      " 'vectors' 'version' 'versions' 'view' 'viewed' 'volume19' 'walk' 'way'\n",
      " 'weak' 'web' 'weight' 'weighted' 'weights' 'weshow' 'wide' 'widely'\n",
      " 'width' 'word' 'words' 'work' 'works' 'world' 'worst' 'years' 'yields'\n",
      " 'zero']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(max_features = 1000, stop_words='english')\n",
    "X = vectorizer.fit_transform([paper[\"abstract\"] for paper in jmlr_papers])\n",
    "print(vectorizer.get_feature_names_out()) # Top-1000 words\n",
    "C = X.toarray() # Count matrix\n",
    "\n",
    "# Removing documents with 0 words\n",
    "idx = np.where(np.sum(C, axis = 1)==0)\n",
    "C = np.delete(C, idx, axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "447e3267",
   "metadata": {},
   "source": [
    "**Q2.** How many elements of $\\mathbf{C}$ are non-zero ? Is this surprising ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b7e800b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 4.53% of non-zero elements in the count matrix\n"
     ]
    }
   ],
   "source": [
    "nb_elements_non_zero = np.sum(C>0)/np.sum(C>=0)\n",
    "print(\"There are\", str(nb_elements_non_zero.round(4)*100)+\"% of non-zero elements in the count matrix\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba38e64",
   "metadata": {},
   "source": [
    "__Remarque__: Ce résultats n'est pas surprenant car les abstracts sont des textes courts donc il y a peu de mots différents dans chaque abstracts surtout qu'on considère que les 1000 mots les plus fréquents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72726d44",
   "metadata": {},
   "source": [
    "### Partie 2 - Inférence variationnelle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ada0b7f",
   "metadata": {},
   "source": [
    "As you know from the lecture, VI aims at maximizing the ELBO. I have prepared for you the function to compute the ELBO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4cb1f3a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import digamma, loggamma\n",
    "\n",
    "def ELBO(L, G, phi, a, e, W):\n",
    "    # Computes the ELBO with the values of the parameters L (Lambda), G (Gamma), and Phi\n",
    "    # a, e are hyperparameters (alpha and eta)\n",
    "    # W are the words (obsereved)\n",
    "    \n",
    "    # L - K x V matrix (variational parameters Lambda)\n",
    "    # G - D x K matrix (variational parameters Gamma)\n",
    "    # phi - List of D elements, each element is a Nd x K matrix (variational parameters Phi)\n",
    "    # a - Scalar > 0 (hyperparameter alpha)\n",
    "    # e - Scalar > 0 (hyperparameter eta)\n",
    "    # W - List of D elements, each element is a Nd x V matrix (observed words)\n",
    "    \n",
    "    e_log_B = (digamma(L).T - digamma(np.sum(L, axis = 1))).T\n",
    "    e_log_T = (digamma(G).T - digamma(np.sum(G, axis = 1))).T\n",
    "    K = L.shape[0]\n",
    "    D = G.shape[0]\n",
    "    V = L.shape[1]\n",
    "    t1 = (e-1)*np.sum(e_log_B)\n",
    "    t2 = (a-1)*np.sum(e_log_T)\n",
    "\n",
    "    phi_s = np.zeros((D,K))\n",
    "    for d in range(0,D):\n",
    "        phi_s[d,:] = np.sum(phi[d], axis = 0)\n",
    "    t3 = np.sum(e_log_T*phi_s)\n",
    "    \n",
    "    tmp = np.zeros((K,V))\n",
    "    for d in range(0,D):\n",
    "        tmp = tmp + np.dot(phi[d].T, W[d])\n",
    "    t4 = np.sum(e_log_B*tmp)\n",
    "    \n",
    "    t5 = np.sum(loggamma(np.sum(L, axis = 1))) - np.sum(loggamma(L)) + np.sum((L-1)*e_log_B)\n",
    "    t6 = np.sum(loggamma(np.sum(G, axis = 1))) - np.sum(loggamma(G)) + np.sum((G-1)*e_log_T)\n",
    "\n",
    "    t7 = 0\n",
    "    for d in range(0,D):\n",
    "        t7 = t7 + np.sum(phi[d]*np.log(phi[d] + np.spacing(1)))\n",
    "\n",
    "    return t1 + t2 + t3 + t4 - t5 - t6 - t7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dfc0302",
   "metadata": {},
   "source": [
    "**Q1.** Transform the matrix $\\mathbf{C}$ into the observed words $\\mathbf{w}$. $\\mathbf{w}$ should be a list of $D$ elements, each element of the list being a $N_d \\times V$ matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5ea0f64c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "def get_w(c):\n",
    "    Nd = np.sum(c > 0)\n",
    "    w = np.zeros((Nd, 1000))\n",
    "    idx = np.where(c > 0)[0] \n",
    "    # parcours des colonnes de w\n",
    "    j = 0\n",
    "    for i in range(len(idx)):\n",
    "        # met 1 sur les lignes de j jusqu'à j+c[idx[i]] et colonne idx[i]\n",
    "        w[j:j + c[idx[i]], idx[i]] = 1\n",
    "        j = j + c[idx[i]]\n",
    "    return w\n",
    "\n",
    "result = get_w(C[0])\n",
    "print(np.sum(result, axis = 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "02618ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "D = C.shape[0] # Number of documents\n",
    "V = C.shape[1] # Number of words\n",
    "#Nd_max = np.max(np.sum(C, axis = 1)) # Maximum number of words in a document\n",
    "#print(\"There are\", str(D)+\" documents,\", str(V)+\" words and\", str(Nd_max)+\" maximum words in a document\")\n",
    "W  = []\n",
    "for d in range(0,D):\n",
    "    W.append(get_w(C[d]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c0aa3d",
   "metadata": {},
   "source": [
    "**Q2.** Implement the CAVI algorithm. The updates are given at the beginning of the notebook. Monitor the convergence with the values of the ELBO (but start with a fixed number of iterations, like 50)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "357e5949",
   "metadata": {},
   "source": [
    "*Check the .pdf file describing the model.**\n",
    "The posterior is :\n",
    "$$p(\\boldsymbol{\\beta}, \\boldsymbol{\\theta}, \\mathbf{z} | \\mathcal{D}),$$\n",
    "which we are going to approximate in the following way :\n",
    "$$\\simeq \\left[ \\prod_{k=1}^K q(\\beta_k) \\right] \\left[ \\prod_{d=1}^D q(\\theta_d) \\right] \\left[ \\prod_{d=1}^D \\prod_{n=1}^{N_d} q(z_{dn}) \\right], $$\n",
    "with :\n",
    "* $q(\\beta_k)$ a Dirichlet distribution (of size V) with parameter $[\\lambda_{k1}, ...,\\lambda_{kV}]$\n",
    "* $q(\\gamma_d)$ a Dirichlet distribution (of size K) with parameter $[\\gamma_{d1}, ...,\\gamma_{dK}]$\n",
    "* $q(z_{dn})$ a Multinomial distribution (of size K) with parameter $[\\phi_{dn1}, ..., \\phi_{dnK}]$\n",
    "\n",
    "The updates are as follows :\n",
    "* $$\\lambda_{kv} = \\eta + \\sum_{d=1}^D \\sum_{n=1}^{N_d} w_{dnv} \\phi_{dnk} $$\n",
    "* $$\\gamma_{dk} = \\alpha + \\sum_{n=1}^{N_d} \\phi_{dnk}$$\n",
    "* $$ \\phi_{dnk} \\propto \\exp \\left( \\Psi(\\gamma_{dk}) + \\Psi(\\lambda_{k, w_{dn}}) - \\Psi(\\sum_{v=1}^V \\lambda_{kv}) \\right)$$\n",
    "\n",
    "$\\Psi$ is the digamma function, use `scipy.special.digamma`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c14607df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import diagmma\n",
    "from scipy.special import digamma\n",
    "from tqdm import tqdm\n",
    "def CAVI(W, K, a, e, seed): # Other arguments may be added\n",
    "    np.random.seed(seed)\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        W (_type_):  taille D * Nd * V\n",
    "        K (_type_): nombre de topics\n",
    "        a (_type_): hyperparamètre alpha\n",
    "        e (_type_): hyperparamètre eta\n",
    "\n",
    "    Returns:\n",
    "        L  : K * V ; valuer des lambda_kv\n",
    "        G : D * K; valeur des gamma_dk\n",
    "        phi : D * Nd * K; valeur des phi_dnk\n",
    "        \n",
    "    \"\"\"\n",
    "    # Initialization\n",
    "    D = len(W)\n",
    "    V = W[0].shape[1]\n",
    " \n",
    "    L = e + np.ones((K, 1000))/K\n",
    "    G = a + np.ones((D, K))/D\n",
    "    phi = [np.ones((W[d].shape[0], K)) for d in range(D)]\n",
    "     \n",
    "    ELBO_value = []\n",
    "    # Iterations\n",
    "    for iter in tqdm(range(10)):\n",
    "        \n",
    "        # update phi\n",
    "    \n",
    "        for d in range(D):\n",
    "            for n in range(W[d].shape[0]):\n",
    "                for k in range(K):\n",
    "                    phi[d][n,k] =  np.exp(digamma(G[d,k])+digamma(L[k,np.where(W[d][n] == 1)[0][0]])\n",
    "                                                                           -digamma(np.sum(G[d])))\n",
    "                phi[d][n,:] = phi[d][n,:]/np.sum(phi[d][n,:])\n",
    "                \n",
    "        # update lambda et gamma\n",
    "        L = e\n",
    "        for d in range(0,D):\n",
    "            L = L + np.dot(phi[d].T, W[d])\n",
    "            G[d] = np.sum(phi[d], axis = 0)\n",
    "         \n",
    "        G += a\n",
    "        #print(L.shape)\n",
    "        #print(G.shape)\n",
    "        \n",
    "        ELBO_value.append(ELBO(L, G, phi, a, e, W))\n",
    "        \n",
    "    return L, G, phi, ELBO_value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b88160d",
   "metadata": {},
   "source": [
    "**Q3.** Run the algorithm with $K = 10$, $\\alpha = 0.5$, $\\eta = 0.1$. From the results, compute the MMSE of $\\lambda_{kv}$ and $\\gamma_{dk}$.\n",
    "\n",
    "**Bonus** : Re-run the algorithm several times with different initializations, and keep the solution which returns the highest ELBO.\n",
    "\n",
    "NB : In my implementation, one iteration of the CAVI algorithm takes about 4 seconds to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "5f54318b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [02:31<00:00, 15.12s/it]\n"
     ]
    }
   ],
   "source": [
    "L, G, phi, ELBO_value = CAVI(W, 10, 0.5, 0.1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "061e2953",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1df98efd2e0>]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(ELBO_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcbc6553",
   "metadata": {},
   "source": [
    "**Q4.** Based on the MMSE estimates :\n",
    "* What are the top-10 words per topic ? With your machine learning knowledge, can you make sense of some of the topics ?\n",
    "* Choose one document at random and display its topic proportions. Comment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "9df9ced5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize L suivant colonne\n",
    "L_norm = L / np.sum(L, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3a570f3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.00000043e-01, 1.00000044e-01, 1.00000039e-01, ...,\n",
       "        1.00000000e-01, 1.00000000e-01, 1.00000000e-01],\n",
       "       [9.50999996e+01, 4.60999996e+01, 1.54100000e+02, ...,\n",
       "        1.00000000e-01, 1.00000000e-01, 1.00000000e-01],\n",
       "       [1.00000046e-01, 1.00000047e-01, 1.00000042e-01, ...,\n",
       "        1.00000000e-01, 1.00000000e-01, 1.00000000e-01],\n",
       "       ...,\n",
       "       [1.00000033e-01, 1.00000034e-01, 1.00000030e-01, ...,\n",
       "        1.00000000e-01, 1.00000000e-01, 1.00000000e-01],\n",
       "       [1.00000031e-01, 1.00000032e-01, 1.00000028e-01, ...,\n",
       "        1.00000000e-01, 1.00000000e-01, 1.00000000e-01],\n",
       "       [1.00000040e-01, 1.00000041e-01, 1.00000037e-01, ...,\n",
       "        1.00000000e-01, 1.00000000e-01, 1.00000000e-01]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ef772fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_words_per_topic(L, vectorizer, k):\n",
    "    \n",
    "    # L - K x V matrix (variational parameters Lambda)\n",
    "    # vectorizer.get_feature_names_out() - List of V words\n",
    "    # 10 - Number of top words to return\n",
    "    # Returns a list of K elements, each element is a list of 10 words\n",
    "    # topic k\n",
    "    top_words = []\n",
    "    indices = np.argsort(L[k])[::-1]\n",
    "    for i in range(10):\n",
    "        top_words.append(vectorizer.get_feature_names_out()[indices[i]])\n",
    "    \n",
    "    return top_words\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d0bb5ad7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['utility',\n",
       " 'work',\n",
       " 'tree',\n",
       " 'term',\n",
       " 'uses',\n",
       " 'trees',\n",
       " 'variational',\n",
       " 'tight',\n",
       " 'validation',\n",
       " 'testing']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_top_words_per_topic(L, vectorizer, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e446f419",
   "metadata": {},
   "source": [
    "----- Your answer here -----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c13019da",
   "metadata": {},
   "source": [
    "**Q5.** Open questions :\n",
    "* What are some limitations of the LDA model ? Can you imagine an improvement ?\n",
    "* In this notebook, we have treated the hyperparameters as fixed. How could they be learned ?\n",
    "* Can you imagine a method to choose the number of topics ?\n",
    "* What strategies should we use to make the algorithm more efficient ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "600e6149",
   "metadata": {},
   "source": [
    "**BONUS.** Papier-crayon. À partir du modèle, pouvez-vous dériver les lois conditionnelles de l'échantillonneur de Gibbs ? Pour rappel, nous avons besoin de ces lois pour dériver ensuite les updates de l'algorithme CAVI."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
